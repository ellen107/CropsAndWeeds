{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaC/5DPHnbl7iEzYD82O0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ellen107/CropsAndWeeds/blob/main/CropWeedClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.mobilenet_v3 import models"
      ],
      "metadata": {
        "id": "hQV9eWU9otvs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ellen107/CropsAndWeeds.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P--fsnW2N3e6",
        "outputId": "8fc96dc8-6e13-442c-dfed-9fca2c68ee65"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CropsAndWeeds'...\n",
            "remote: Enumerating objects: 734, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 734 (delta 8), reused 0 (delta 0), pack-reused 706\u001b[K\n",
            "Receiving objects: 100% (734/734), 1.19 GiB | 42.44 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "Updating files: 100% (1169/1169), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/CropsAndWeeds/output'\n",
        "train_dir = '/content/CropsAndWeeds/output/train'\n",
        "train_crop_dir = '/content/CropsAndWeeds/output/train/crops'\n",
        "train_weed_dir = '/content/CropsAndWeeds/output/train/weeds'\n",
        "test_dir = '/content/CropsAndWeeds/output/test'\n",
        "test_crop_dir = '/content/CropsAndWeeds/output/test/crops'\n",
        "test_weed_dir = '/content/CropsAndWeeds/output/test/weeds'\n",
        "valid_dir = '/content/CropsAndWeeds/output/val'\n",
        "valid_crop_dir = '/content/CropsAndWeeds/output/val/crops'\n",
        "valid_weed_dir = '/content/CropsAndWeeds/output/val/weeds'"
      ],
      "metadata": {
        "id": "qbZI1SeGowxk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_crop_train = len(os.listdir(train_crop_dir))\n",
        "num_weed_train = len(os.listdir(train_weed_dir))\n",
        "num_crop_validaition = len(os.listdir(valid_crop_dir))\n",
        "num_weed_validation= len(os.listdir(valid_weed_dir))\n",
        "num_crop_test = len(os.listdir(test_crop_dir))\n",
        "num_weed_test= len(os.listdir(test_weed_dir))"
      ],
      "metadata": {
        "id": "p_wuG5eI_gke"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Training Crop Images\",num_crop_train)\n",
        "print(\"Total Training Weed Images\",num_weed_train)\n",
        "print(\"--\")\n",
        "print(\"Total validation Crop Images\",num_crop_validaition)\n",
        "print(\"Total validation Weed Images\",num_weed_validation)\n",
        "print(\"--\")\n",
        "print(\"Total Test Crop Images\", num_crop_test)\n",
        "print(\"Total Test Weed Images\",num_weed_test)\n",
        "total_train = num_crop_train+num_weed_train\n",
        "total_validation = num_crop_validaition+num_weed_validation\n",
        "total_test = num_crop_test+num_weed_test\n",
        "print(\"--\")\n",
        "print(\"Total Training Images\",total_train)\n",
        "print(\"--\")\n",
        "print(\"Total Validation Images\",total_validation)\n",
        "print(\"--\")\n",
        "print(\"Total Testing Images\",total_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JKdDAnE_5Qu",
        "outputId": "366baf07-1dbe-4433-c419-2ceefe8c4205"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Training Crop Images 199\n",
            "Total Training Weed Images 256\n",
            "--\n",
            "Total validation Crop Images 24\n",
            "Total validation Weed Images 32\n",
            "--\n",
            "Total Test Crop Images 27\n",
            "Total Test Weed Images 36\n",
            "--\n",
            "Total Training Images 455\n",
            "--\n",
            "Total Validation Images 56\n",
            "--\n",
            "Total Testing Images 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE  = 224\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "DNvDA1_aqh3M"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_gen_train = ImageDataGenerator(rescale = 1./255)\n",
        "train_data_gen = image_gen_train.flow_from_directory(batch_size = batch_size,\n",
        "directory = train_dir,\n",
        "shuffle= True,\n",
        "target_size = (IMG_SHAPE,IMG_SHAPE),\n",
        "class_mode = 'binary')\n",
        "image_generator_validation = ImageDataGenerator(rescale=1./255)\n",
        "val_data_gen = image_generator_validation.flow_from_directory(batch_size=batch_size,\n",
        "directory=valid_dir,\n",
        "target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "class_mode='binary')\n",
        "image_gen_test = ImageDataGenerator(rescale=1./255)\n",
        "test_data_gen = image_gen_test.flow_from_directory(batch_size=batch_size,\n",
        "directory=test_dir,\n",
        "target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "class_mode='binary')"
      ],
      "metadata": {
        "id": "vfGbNBDRqisS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a8d698-6115-49a5-9b51-c12a2acc3850"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 455 images belonging to 2 classes.\n",
            "Found 56 images belonging to 2 classes.\n",
            "Found 63 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model = tf.keras.applications.VGG16(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")"
      ],
      "metadata": {
        "id": "QGgTXsmWqlNP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in pre_trained_model.layers:\n",
        "  print(layer.name)\n",
        "  layer.trainable = False  #freeze the training layers of VGG-16"
      ],
      "metadata": {
        "id": "KxkUvVwAqoFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20009540-5037-42c4-d9cd-6eb5af004657"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_2\n",
            "block1_conv1\n",
            "block1_conv2\n",
            "block1_pool\n",
            "block2_conv1\n",
            "block2_conv2\n",
            "block2_pool\n",
            "block3_conv1\n",
            "block3_conv2\n",
            "block3_conv3\n",
            "block3_pool\n",
            "block4_conv1\n",
            "block4_conv2\n",
            "block4_conv3\n",
            "block4_pool\n",
            "block5_conv1\n",
            "block5_conv2\n",
            "block5_conv3\n",
            "block5_pool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_layer = pre_trained_model.get_layer('block5_pool')\n",
        "last_output = last_layer.output\n",
        "x = tf.keras.layers.GlobalMaxPooling2D()(last_output)\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.Dense(2, activation='sigmoid')(x)"
      ],
      "metadata": {
        "id": "3fPXcCsMqqY1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.keras.layers.Dense(3, activation='softmax')(x)"
      ],
      "metadata": {
        "id": "L9HTgv1Zqt11"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(pre_trained_model.input,x)"
      ],
      "metadata": {
        "id": "Trk7XTTRqug6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])"
      ],
      "metadata": {
        "id": "1SsHEp52qw4h"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.mobilenet_v3 import models\n",
        "vgg_classifier = model.fit(train_data_gen,\n",
        "steps_per_epoch=(total_train//batch_size),\n",
        "epochs = 5,validation_data=val_data_gen,\n",
        "validation_steps=(total_validation//batch_size),\n",
        "batch_size = batch_size,\n",
        "verbose = 1)"
      ],
      "metadata": {
        "id": "bIu-tJTyq0pe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "39c1d9d6-573b-4ead-8cba-60e686de0f85"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f8a120a43dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobilenet_v3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m vgg_classifier = model.fit(train_data_gen,\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_train\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_validation\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccept_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m     raise UnidentifiedImageError(\n\u001b[0m\u001b[1;32m   3031\u001b[0m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m     )\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7fcba80c92c0>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zi5U6OowuuqG"
      }
    }
  ]
}